{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3f23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import cv2\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7a9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretórios\n",
    "train_dir = 'dataset/processado/train/'\n",
    "val_dir = 'dataset/processado/validation/'\n",
    "test_dir = 'dataset/processado/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c62e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patient_id(filename):\n",
    "    \"\"\"Extrai o ID do paciente do nome do arquivo\"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    # Formato esperado: UID_X_Y_Z_all.bmp ou semelhante\n",
    "    try:\n",
    "        patient_id = basename.split('_')[1].split('-')[0]\n",
    "        return patient_id\n",
    "    except:\n",
    "        # Fallback para outros formatos\n",
    "        return basename.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b90db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_images_and_patients(directory):\n",
    "    \"\"\"Coleta todas as imagens e seus respectivos IDs de pacientes\"\"\"\n",
    "    images = []\n",
    "    patients = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.jpg', '.png', '.bmp')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                patient_id = extract_patient_id(file)\n",
    "                \n",
    "                images.append(img_path)\n",
    "                patients.append(patient_id)\n",
    "    \n",
    "    return np.array(images), np.array(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d589d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_proportion(unique_ids, proportion, random_state):\n",
    "    \"\"\"Divide uma lista de IDs únicos pela proporção dada\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(unique_ids)\n",
    "    \n",
    "    split_idx = int(len(unique_ids) * proportion)\n",
    "    return unique_ids[:split_idx], unique_ids[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c348e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(image_paths, target_dir):\n",
    "    \"\"\"Copia as imagens para o diretório de destino\"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=f\"Copiando para {os.path.basename(target_dir)}\"):\n",
    "        shutil.copy2(img_path, os.path.join(target_dir, os.path.basename(img_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eadc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_augmentation_per_patient(patient_ids, images, all_patients, total_needed):\n",
    "    \"\"\"Calcula quantas imagens augmentadas são necessárias por paciente\"\"\"\n",
    "    # Mapear pacientes para imagens\n",
    "    patient_to_images = {}\n",
    "    for img, patient in zip(images, all_patients):\n",
    "        if patient in patient_ids:\n",
    "            if patient not in patient_to_images:\n",
    "                patient_to_images[patient] = []\n",
    "            patient_to_images[patient].append(img)\n",
    "    \n",
    "    # Calcular a proporção de imagens por paciente\n",
    "    total_images = len(images)\n",
    "    augmentation_per_patient = {}\n",
    "    \n",
    "    for patient in patient_ids:\n",
    "        patient_images = patient_to_images.get(patient, [])\n",
    "        # Calcular a proporção do paciente\n",
    "        proportion = len(patient_images) / total_images if total_images > 0 else 0\n",
    "        # Calcular o número de imagens augmentadas\n",
    "        aug_count = max(5, int(proportion * total_needed))\n",
    "        augmentation_per_patient[patient] = aug_count\n",
    "    \n",
    "    # Ajustar para garantir o total correto\n",
    "    total_augmentation = sum(augmentation_per_patient.values())\n",
    "    if total_augmentation < total_needed:\n",
    "        # Distribuir a diferença entre os pacientes\n",
    "        difference = total_needed - total_augmentation\n",
    "        # Adicionar a diferença aos pacientes com mais imagens originais\n",
    "        sorted_patients = sorted(patient_ids, \n",
    "                                key=lambda p: len(patient_to_images.get(p, [])), \n",
    "                                reverse=True)\n",
    "        \n",
    "        for i, patient in enumerate(sorted_patients):\n",
    "            if i >= difference:\n",
    "                break\n",
    "            augmentation_per_patient[patient] += 1\n",
    "    \n",
    "    return augmentation_per_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f0846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation_by_patient(images, all_patients, target_patients, aug_per_patient, output_dir, aggressive=False):\n",
    "    \"\"\"Aplica data augmentation por paciente\"\"\"\n",
    "    # Agrupar imagens por paciente\n",
    "    patient_to_images = {}\n",
    "    for img, patient in zip(images, all_patients):\n",
    "        if patient in target_patients:\n",
    "            if patient not in patient_to_images:\n",
    "                patient_to_images[patient] = []\n",
    "            patient_to_images[patient].append(img)\n",
    "    \n",
    "    # Aplicar augmentation para cada paciente\n",
    "    for patient, imgs in tqdm(patient_to_images.items(), desc=\"Augmentação por paciente\"):\n",
    "        target_aug_count = aug_per_patient.get(patient, 0)\n",
    "        if target_aug_count <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Distribuir augmentações entre as imagens originais\n",
    "        imgs_needed_per_original = max(1, target_aug_count // len(imgs))\n",
    "        \n",
    "        aug_count = 0\n",
    "        aug_round = 0\n",
    "        \n",
    "        # Continuar aplicando rounds de augmentação até atingir o número necessário\n",
    "        while aug_count < target_aug_count:\n",
    "            aug_round += 1\n",
    "            for img_path in imgs:\n",
    "                # Ler imagem\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                \n",
    "                basename = os.path.basename(img_path)\n",
    "                name_without_ext = os.path.splitext(basename)[0]\n",
    "                \n",
    "                # Determinar o número de augmentações para esta imagem neste round\n",
    "                n_augs = min(imgs_needed_per_original, target_aug_count - aug_count)\n",
    "                if n_augs <= 0:\n",
    "                    break\n",
    "                \n",
    "                # Aplicar mais transformações se aggressive=True\n",
    "                n_transforms = random.randint(3, 5) if aggressive else random.randint(1, 3)\n",
    "                \n",
    "                # Gerar augmentações\n",
    "                for i in range(n_augs):\n",
    "                    # Aplicar transformações aleatórias\n",
    "                    aug_img = apply_random_augmentation(img, n_transforms=n_transforms)\n",
    "                    \n",
    "                    # Salvar imagem augmentada\n",
    "                    aug_filename = f\"{name_without_ext}_aug_r{aug_round}_{i}.png\"\n",
    "                    cv2.imwrite(os.path.join(output_dir, aug_filename), aug_img)\n",
    "                    \n",
    "                    aug_count += 1\n",
    "                    if aug_count >= target_aug_count:\n",
    "                        break\n",
    "                \n",
    "                if aug_count >= target_aug_count:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b473a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_augmentation(image, n_transforms=2):\n",
    "    \"\"\"Aplica transformações aleatórias na imagem\"\"\"\n",
    "    # Lista de possíveis augmentações\n",
    "    augmentations = [\n",
    "        lambda img: cv2.flip(img, 1),  # Espelhamento horizontal\n",
    "        lambda img: cv2.flip(img, 0),  # Espelhamento vertical\n",
    "        lambda img: cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE),  # Rotação 90°\n",
    "        lambda img: cv2.rotate(img, cv2.ROTATE_180),  # Rotação 180°\n",
    "        lambda img: cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE),  # Rotação 270°\n",
    "        lambda img: cv2.GaussianBlur(img, (5, 5), 0),  # Blur gaussiano\n",
    "        lambda img: add_salt_pepper_noise(img, 0.02),  # Ruído sal e pimenta\n",
    "        lambda img: apply_shear(img, random.uniform(0.1, 0.3)),  # Cisalhamento\n",
    "        lambda img: adjust_brightness(img, factor=random.uniform(0.8, 1.2)),  # Ajuste de brilho\n",
    "        lambda img: adjust_contrast(img, factor=random.uniform(0.8, 1.2)),  # Ajuste de contraste\n",
    "        lambda img: apply_random_crop(img, crop_percent=random.uniform(0.8, 0.95)),  # Corte aleatório\n",
    "        lambda img: apply_elastic_transform(img, alpha=random.randint(40, 60), sigma=random.randint(4, 6))  # Transformação elástica\n",
    "    ]\n",
    "    \n",
    "    # Escolher quantas augmentações aplicar\n",
    "    num_augs = min(n_transforms, len(augmentations))\n",
    "    \n",
    "    # Escolher augmentações aleatórias\n",
    "    selected_augs = random.sample(augmentations, num_augs)\n",
    "    \n",
    "    # Aplicar augmentações\n",
    "    result = image.copy()\n",
    "    for aug_func in selected_augs:\n",
    "        result = aug_func(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00284dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salt_pepper_noise(image, amount):\n",
    "    \"\"\"Adiciona ruído sal e pimenta na imagem\"\"\"\n",
    "    output = image.copy()\n",
    "    h, w = image.shape[:2]\n",
    "    num_salt = int(amount * image.size * 0.5)\n",
    "    num_pepper = int(amount * image.size * 0.5)\n",
    "    \n",
    "    # Salt\n",
    "    coords = [np.random.randint(0, i - 1, num_salt) for i in (h, w)]\n",
    "    output[coords[0], coords[1]] = 255\n",
    "    \n",
    "    # Pepper\n",
    "    coords = [np.random.randint(0, i - 1, num_pepper) for i in (h, w)]\n",
    "    output[coords[0], coords[1]] = 0\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dbc6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_shear(image, factor):\n",
    "    \"\"\"Aplica transformação de cisalhamento na imagem\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Matriz de transformação\n",
    "    M = np.float32([[1, factor, 0], [0, 1, 0]])\n",
    "    \n",
    "    # Aplicar transformação\n",
    "    sheared = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "    return sheared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e297aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_brightness(image, factor):\n",
    "    \"\"\"Ajusta o brilho da imagem\"\"\"\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hsv = hsv.astype(np.float32)\n",
    "    \n",
    "    # Ajustar o canal V (brilho)\n",
    "    hsv[:, :, 2] = hsv[:, :, 2] * factor\n",
    "    hsv[:, :, 2] = np.clip(hsv[:, :, 2], 0, 255)\n",
    "    \n",
    "    hsv = hsv.astype(np.uint8)\n",
    "    result = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cababb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_contrast(image, factor):\n",
    "    \"\"\"Ajusta o contraste da imagem\"\"\"\n",
    "    mean = np.mean(image, axis=(0, 1))\n",
    "    result = (image.astype(np.float32) - mean) * factor + mean\n",
    "    result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae939e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_crop(image, crop_percent=0.9):\n",
    "    \"\"\"Aplica corte aleatório e redimensiona para o tamanho original\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Calcular dimensões do corte\n",
    "    crop_h = int(h * crop_percent)\n",
    "    crop_w = int(w * crop_percent)\n",
    "    \n",
    "    # Calcular posição do corte\n",
    "    start_h = random.randint(0, h - crop_h)\n",
    "    start_w = random.randint(0, w - crop_w)\n",
    "    \n",
    "    # Cortar imagem\n",
    "    cropped = image[start_h:start_h+crop_h, start_w:start_w+crop_w]\n",
    "    \n",
    "    # Redimensionar para o tamanho original\n",
    "    resized = cv2.resize(cropped, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95c69a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_elastic_transform(image, alpha=50, sigma=5, random_state=None):\n",
    "    \"\"\"Aplica transformação elástica à imagem\"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "    \n",
    "    shape = image.shape\n",
    "    dx = random_state.rand(shape[0], shape[1]) * 2 - 1\n",
    "    dy = random_state.rand(shape[0], shape[1]) * 2 - 1\n",
    "    dx = cv2.GaussianBlur(dx, (0, 0), sigma) * alpha\n",
    "    dy = cv2.GaussianBlur(dy, (0, 0), sigma) * alpha\n",
    "    \n",
    "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "    map_x = np.float32(x + dx)\n",
    "    map_y = np.float32(y + dy)\n",
    "    \n",
    "    return cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79617f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_directory(train_dir, val_dir, test_dir):\n",
    "    \"\"\"Conta o número de imagens em cada diretório\"\"\"\n",
    "    # Treino\n",
    "    all_train_count = len([f for f in os.listdir(os.path.join(train_dir, 'all')) \n",
    "                           if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "    hem_train_count = len([f for f in os.listdir(os.path.join(train_dir, 'hem')) \n",
    "                           if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "    \n",
    "    # Validação\n",
    "    all_val_count = len([f for f in os.listdir(os.path.join(val_dir, 'all')) \n",
    "                         if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "    hem_val_count = len([f for f in os.listdir(os.path.join(val_dir, 'hem')) \n",
    "                         if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "    \n",
    "    # Teste\n",
    "    all_test_count = len([f for f in os.listdir(os.path.join(test_dir, 'all')) \n",
    "                          if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "    hem_test_count = len([f for f in os.listdir(os.path.join(test_dir, 'hem')) \n",
    "                          if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "    \n",
    "    print(\"\\nContagem final de imagens:\")\n",
    "    print(f\"Treino - ALL: {all_train_count}, HEM: {hem_train_count}, Total: {all_train_count + hem_train_count}\")\n",
    "    print(f\"Validação - ALL: {all_val_count}, HEM: {hem_val_count}, Total: {all_val_count + hem_val_count}\")\n",
    "    print(f\"Teste - ALL: {all_test_count}, HEM: {hem_test_count}, Total: {all_test_count + hem_test_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "888dcd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(train_dir, val_dir, test_dir):\n",
    "    \"\"\"Gera gráficos para visualizar a distribuição de imagens por paciente\"\"\"\n",
    "    # Criar figura para distribuição por paciente\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Treino - ALL\n",
    "    train_all_files = [f for f in os.listdir(os.path.join(train_dir, 'all')) \n",
    "                       if f.endswith(('.jpg', '.png', '.bmp'))]\n",
    "    train_all_patients = [extract_patient_id(f) for f in train_all_files]\n",
    "    train_all_count = Counter(train_all_patients)\n",
    "    \n",
    "    # Treino - HEM\n",
    "    train_hem_files = [f for f in os.listdir(os.path.join(train_dir, 'hem')) \n",
    "                       if f.endswith(('.jpg', '.png', '.bmp'))]\n",
    "    train_hem_patients = [extract_patient_id(f) for f in train_hem_files]\n",
    "    train_hem_count = Counter(train_hem_patients)\n",
    "    \n",
    "    # Validação - ALL\n",
    "    val_all_files = [f for f in os.listdir(os.path.join(val_dir, 'all')) \n",
    "                     if f.endswith(('.jpg', '.png', '.bmp'))]\n",
    "    val_all_patients = [extract_patient_id(f) for f in val_all_files]\n",
    "    val_all_count = Counter(val_all_patients)\n",
    "    \n",
    "    # Validação - HEM\n",
    "    val_hem_files = [f for f in os.listdir(os.path.join(val_dir, 'hem')) \n",
    "                     if f.endswith(('.jpg', '.png', '.bmp'))]\n",
    "    val_hem_patients = [extract_patient_id(f) for f in val_hem_files]\n",
    "    val_hem_count = Counter(val_hem_patients)\n",
    "    \n",
    "    # Teste - ALL\n",
    "    test_all_files = [f for f in os.listdir(os.path.join(test_dir, 'all')) \n",
    "                      if f.endswith(('.jpg', '.png', '.bmp'))]\n",
    "    test_all_patients = [extract_patient_id(f) for f in test_all_files]\n",
    "    test_all_count = Counter(test_all_patients)\n",
    "    \n",
    "    # Teste - HEM\n",
    "    test_hem_files = [f for f in os.listdir(os.path.join(test_dir, 'hem')) \n",
    "                      if f.endswith(('.jpg', '.png', '.bmp'))]\n",
    "    test_hem_patients = [extract_patient_id(f) for f in test_hem_files]\n",
    "    test_hem_count = Counter(test_hem_patients)\n",
    "    \n",
    "    # Plotar distribuição por paciente\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.bar(train_all_count.keys(), train_all_count.values())\n",
    "    plt.title('Treino - ALL: Imagens por Paciente')\n",
    "    plt.xlabel('ID do Paciente')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.bar(train_hem_count.keys(), train_hem_count.values())\n",
    "    plt.title('Treino - HEM: Imagens por Paciente')\n",
    "    plt.xlabel('ID do Paciente')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.bar(val_all_count.keys(), val_all_count.values())\n",
    "    plt.title('Validação - ALL: Imagens por Paciente')\n",
    "    plt.xlabel('ID do Paciente')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.bar(val_hem_count.keys(), val_hem_count.values())\n",
    "    plt.title('Validação - HEM: Imagens por Paciente')\n",
    "    plt.xlabel('ID do Paciente')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.bar(test_all_count.keys(), test_all_count.values())\n",
    "    plt.title('Teste - ALL: Imagens por Paciente')\n",
    "    plt.xlabel('ID do Paciente')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.bar(test_hem_count.keys(), test_hem_count.values())\n",
    "    plt.title('Teste - HEM: Imagens por Paciente')\n",
    "    plt.xlabel('ID do Paciente')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribuicao_imagens_por_paciente.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Criar figura para resumo de distribuição de classes\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Totais por conjunto\n",
    "    train_counts = [len(train_all_files), len(train_hem_files)]\n",
    "    val_counts = [len(val_all_files), len(val_hem_files)]\n",
    "    test_counts = [len(test_all_files), len(test_hem_files)]\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(['ALL', 'HEM'], [sum(train_counts), sum(val_counts) + sum(test_counts)])\n",
    "    plt.title('Distribuição de Classes (Treino vs. Validação+Teste)')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(['Treino', 'Validação', 'Teste'], \n",
    "            [sum(train_counts), sum(val_counts), sum(test_counts)])\n",
    "    plt.title('Distribuição por Conjunto (Total)')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    width = 0.35\n",
    "    x = np.arange(3)\n",
    "    plt.bar(x - width/2, [train_counts[0], val_counts[0], test_counts[0]], width, label='ALL')\n",
    "    plt.bar(x + width/2, [train_counts[1], val_counts[1], test_counts[1]], width, label='HEM')\n",
    "    plt.xticks(x, ['Treino', 'Validação', 'Teste'])\n",
    "    plt.title('Distribuição por Conjunto e Classe')\n",
    "    plt.ylabel('Número de Imagens')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.pie([sum(train_counts), sum(val_counts), sum(test_counts)], \n",
    "            labels=['Treino', 'Validação', 'Teste'],\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('Proporção dos Conjuntos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('resumo_distribuicao_dataset.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0b83df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coletando informações sobre imagens e pacientes...\n",
      "ALL: 7272 imagens de 47 pacientes\n",
      "HEM: 3389 imagens de 26 pacientes\n",
      "\n",
      "Dividindo pacientes em conjuntos...\n",
      "ALL - Treino: 32 pacientes, Validação: 10 pacientes, Teste: 5 pacientes\n",
      "HEM - Treino: 18 pacientes, Validação: 5 pacientes, Teste: 3 pacientes\n",
      "\n",
      "Distribuindo imagens originais...\n",
      "ALL - Treino: 4836 imagens, Validação: 1716 imagens, Teste: 720 imagens\n",
      "HEM - Treino: 2580 imagens, Validação: 582 imagens, Teste: 227 imagens\n",
      "\n",
      "Copiando imagens originais para seus diretórios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copiando para all: 100%|██████████| 4836/4836 [02:32<00:00, 31.79it/s] \n",
      "Copiando para all: 100%|██████████| 1716/1716 [00:52<00:00, 32.56it/s] \n",
      "Copiando para all: 100%|██████████| 720/720 [00:21<00:00, 33.87it/s] \n",
      "Copiando para hem: 100%|██████████| 2580/2580 [01:19<00:00, 32.26it/s] \n",
      "Copiando para hem: 100%|██████████| 582/582 [00:17<00:00, 33.22it/s] \n",
      "Copiando para hem: 100%|██████████| 227/227 [00:08<00:00, 27.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aplicando data augmentation para balancear o conjunto de treinamento...\n",
      "ALL: Gerando 5164 imagens adicionais para treinamento\n",
      "HEM: Gerando 7420 imagens adicionais para treinamento\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmentação por paciente: 100%|██████████| 26/26 [01:52<00:00,  4.33s/it]\n",
      "Augmentação por paciente: 100%|██████████| 11/11 [02:24<00:00, 13.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aplicando data augmentation para o conjunto de validação...\n",
      "ALL: Gerando 284 imagens adicionais para validação\n",
      "HEM: Gerando 1418 imagens adicionais para validação\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmentação por paciente: 100%|██████████| 2/2 [00:00<00:00,  2.17it/s]\n",
      "Augmentação por paciente: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aplicando data augmentation para o conjunto de teste...\n",
      "ALL: Gerando 280 imagens adicionais para teste\n",
      "HEM: Gerando 773 imagens adicionais para teste\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmentação por paciente: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Augmentação por paciente: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando números finais...\n",
      "\n",
      "Contagem final de imagens:\n",
      "Treino - ALL: 8292, HEM: 8092, Total: 16384\n",
      "Validação - ALL: 1744, HEM: 582, Total: 2326\n",
      "Teste - ALL: 749, HEM: 227, Total: 976\n",
      "\n",
      "Gerando visualização das distribuições...\n",
      "\n",
      "Preparação do dataset concluída com sucesso!\n",
      "Dataset preparado com sucesso:\n",
      "- Treino: dataset/processado/train\n",
      "- Validação: dataset/processado/validation\n",
      "- Teste: dataset/processado/test\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_preparation_pipeline(\n",
    "    all_dir,            # Diretório com imagens ALL (câncer)\n",
    "    hem_dir,            # Diretório com imagens HEM (saudáveis)\n",
    "    output_dir,         # Diretório de saída\n",
    "    train_size=0.7,     # Proporção para treinamento\n",
    "    val_size=0.2,       # Proporção para validação\n",
    "    test_size=0.1,      # Proporção para teste\n",
    "    target_train_per_class=10000,  # Alvo para cada classe após augmentação no treino\n",
    "    target_val_per_class=2000,     # Alvo para cada classe após augmentação na validação\n",
    "    target_test_per_class=1000,    # Alvo para cada classe após augmentação no teste\n",
    "    random_state=42     # Semente aleatória para reprodutibilidade\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo para preparação do dataset:\n",
    "    1. Separação por paciente\n",
    "    2. Balanceamento das classes\n",
    "    3. Data augmentation\n",
    "    \"\"\"\n",
    "    # Criar diretórios de saída\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_dir = os.path.join(output_dir, 'train')\n",
    "    val_dir = os.path.join(output_dir, 'validation')\n",
    "    test_dir = os.path.join(output_dir, 'test')\n",
    "    \n",
    "    for d in [train_dir, val_dir, test_dir]:\n",
    "        os.makedirs(os.path.join(d, 'all'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(d, 'hem'), exist_ok=True)\n",
    "    \n",
    "    # 1. Coletar informações sobre imagens e pacientes\n",
    "    print(\"Coletando informações sobre imagens e pacientes...\")\n",
    "    all_images, all_patients = collect_images_and_patients(all_dir)\n",
    "    hem_images, hem_patients = collect_images_and_patients(hem_dir)\n",
    "    \n",
    "    print(f\"ALL: {len(all_images)} imagens de {len(np.unique(all_patients))} pacientes\")\n",
    "    print(f\"HEM: {len(hem_images)} imagens de {len(np.unique(hem_patients))} pacientes\")\n",
    "    \n",
    "    # 2. Dividir pacientes em conjuntos de treino, validação e teste\n",
    "    print(\"\\nDividindo pacientes em conjuntos...\")\n",
    "    \n",
    "    # ALL - Divisão treino/validação+teste\n",
    "    all_train_patients, all_temp_patients = split_by_proportion(\n",
    "        np.unique(all_patients), train_size, random_state)\n",
    "    \n",
    "    # ALL - Divisão validação/teste\n",
    "    val_proportion = val_size / (val_size + test_size)\n",
    "    all_val_patients, all_test_patients = split_by_proportion(\n",
    "        all_temp_patients, val_proportion, random_state)\n",
    "    \n",
    "    # HEM - Divisão treino/validação+teste\n",
    "    hem_train_patients, hem_temp_patients = split_by_proportion(\n",
    "        np.unique(hem_patients), train_size, random_state)\n",
    "    \n",
    "    # HEM - Divisão validação/teste\n",
    "    hem_val_patients, hem_test_patients = split_by_proportion(\n",
    "        hem_temp_patients, val_proportion, random_state)\n",
    "    \n",
    "    print(f\"ALL - Treino: {len(all_train_patients)} pacientes, Validação: {len(all_val_patients)} pacientes, Teste: {len(all_test_patients)} pacientes\")\n",
    "    print(f\"HEM - Treino: {len(hem_train_patients)} pacientes, Validação: {len(hem_val_patients)} pacientes, Teste: {len(hem_test_patients)} pacientes\")\n",
    "    \n",
    "    # 3. Distribuir imagens originais com base na divisão de pacientes\n",
    "    print(\"\\nDistribuindo imagens originais...\")\n",
    "    \n",
    "    # ALL\n",
    "    all_train_imgs = [img for img, patient in zip(all_images, all_patients) if patient in all_train_patients]\n",
    "    all_val_imgs = [img for img, patient in zip(all_images, all_patients) if patient in all_val_patients]\n",
    "    all_test_imgs = [img for img, patient in zip(all_images, all_patients) if patient in all_test_patients]\n",
    "    \n",
    "    # HEM\n",
    "    hem_train_imgs = [img for img, patient in zip(hem_images, hem_patients) if patient in hem_train_patients]\n",
    "    hem_val_imgs = [img for img, patient in zip(hem_images, hem_patients) if patient in hem_val_patients]\n",
    "    hem_test_imgs = [img for img, patient in zip(hem_images, hem_patients) if patient in hem_test_patients]\n",
    "    \n",
    "    print(f\"ALL - Treino: {len(all_train_imgs)} imagens, Validação: {len(all_val_imgs)} imagens, Teste: {len(all_test_imgs)} imagens\")\n",
    "    print(f\"HEM - Treino: {len(hem_train_imgs)} imagens, Validação: {len(hem_val_imgs)} imagens, Teste: {len(hem_test_imgs)} imagens\")\n",
    "    \n",
    "    # 4. Copiar imagens originais para os diretórios correspondentes\n",
    "    print(\"\\nCopiando imagens originais para seus diretórios...\")\n",
    "    \n",
    "    # ALL\n",
    "    copy_images(all_train_imgs, os.path.join(train_dir, 'all'))\n",
    "    copy_images(all_val_imgs, os.path.join(val_dir, 'all'))\n",
    "    copy_images(all_test_imgs, os.path.join(test_dir, 'all'))\n",
    "    \n",
    "    # HEM\n",
    "    copy_images(hem_train_imgs, os.path.join(train_dir, 'hem'))\n",
    "    copy_images(hem_val_imgs, os.path.join(val_dir, 'hem'))\n",
    "    copy_images(hem_test_imgs, os.path.join(test_dir, 'hem'))\n",
    "    \n",
    "    # 5. Aplicar data augmentation nas imagens de treinamento\n",
    "    print(\"\\nAplicando data augmentation para balancear o conjunto de treinamento...\")\n",
    "    \n",
    "    # Calcular quantas imagens augmentadas são necessárias para cada classe\n",
    "    all_needed = max(0, target_train_per_class - len(all_train_imgs))\n",
    "    hem_needed = max(0, target_train_per_class - len(hem_train_imgs))\n",
    "    \n",
    "    print(f\"ALL: Gerando {all_needed} imagens adicionais para treinamento\")\n",
    "    print(f\"HEM: Gerando {hem_needed} imagens adicionais para treinamento\")\n",
    "    \n",
    "    if all_needed > 0:\n",
    "        all_per_patient = calculate_augmentation_per_patient(all_train_patients, all_train_imgs, all_patients, all_needed)\n",
    "        apply_augmentation_by_patient(all_train_imgs, all_patients, all_train_patients, \n",
    "                                      all_per_patient, os.path.join(train_dir, 'all'), \n",
    "                                      aggressive=True)\n",
    "    \n",
    "    if hem_needed > 0:\n",
    "        hem_per_patient = calculate_augmentation_per_patient(hem_train_patients, hem_train_imgs, hem_patients, hem_needed)\n",
    "        apply_augmentation_by_patient(hem_train_imgs, hem_patients, hem_train_patients, \n",
    "                                      hem_per_patient, os.path.join(train_dir, 'hem'),\n",
    "                                      aggressive=True)\n",
    "    \n",
    "    # 6. Aplicar data augmentation nas imagens de validação\n",
    "    print(\"\\nAplicando data augmentation para o conjunto de validação...\")\n",
    "    \n",
    "    # Calcular quantas imagens augmentadas são necessárias para validação\n",
    "    all_val_needed = max(0, target_val_per_class - len(all_val_imgs))\n",
    "    hem_val_needed = max(0, target_val_per_class - len(hem_val_imgs))\n",
    "    \n",
    "    print(f\"ALL: Gerando {all_val_needed} imagens adicionais para validação\")\n",
    "    print(f\"HEM: Gerando {hem_val_needed} imagens adicionais para validação\")\n",
    "    \n",
    "    if all_val_needed > 0:\n",
    "        all_val_per_patient = calculate_augmentation_per_patient(all_val_patients, all_val_imgs, all_patients, all_val_needed)\n",
    "        apply_augmentation_by_patient(all_val_imgs, all_patients, all_val_patients, \n",
    "                                      all_val_per_patient, os.path.join(val_dir, 'all'),\n",
    "                                      aggressive=True)\n",
    "    \n",
    "    if hem_val_needed > 0:\n",
    "        hem_val_per_patient = calculate_augmentation_per_patient(hem_val_patients, hem_val_imgs, hem_patients, hem_val_needed)\n",
    "        apply_augmentation_by_patient(hem_val_imgs, hem_patients, hem_val_patients, \n",
    "                                      hem_val_per_patient, os.path.join(val_dir, 'hem'),\n",
    "                                      aggressive=True)\n",
    "    \n",
    "    # 7. Aplicar data augmentation nas imagens de teste\n",
    "    print(\"\\nAplicando data augmentation para o conjunto de teste...\")\n",
    "    \n",
    "    # Calcular quantas imagens augmentadas são necessárias para teste\n",
    "    all_test_needed = max(0, target_test_per_class - len(all_test_imgs))\n",
    "    hem_test_needed = max(0, target_test_per_class - len(hem_test_imgs))\n",
    "    \n",
    "    print(f\"ALL: Gerando {all_test_needed} imagens adicionais para teste\")\n",
    "    print(f\"HEM: Gerando {hem_test_needed} imagens adicionais para teste\")\n",
    "    \n",
    "    if all_test_needed > 0:\n",
    "        all_test_per_patient = calculate_augmentation_per_patient(all_test_patients, all_test_imgs, all_patients, all_test_needed)\n",
    "        apply_augmentation_by_patient(all_test_imgs, all_patients, all_test_patients, \n",
    "                                     all_test_per_patient, os.path.join(test_dir, 'all'),\n",
    "                                     aggressive=True)\n",
    "    \n",
    "    if hem_test_needed > 0:\n",
    "        hem_test_per_patient = calculate_augmentation_per_patient(hem_test_patients, hem_test_imgs, hem_patients, hem_test_needed)\n",
    "        apply_augmentation_by_patient(hem_test_imgs, hem_patients, hem_test_patients, \n",
    "                                     hem_test_per_patient, os.path.join(test_dir, 'hem'),\n",
    "                                     aggressive=True)\n",
    "    \n",
    "    # 8. Verificar resultados finais\n",
    "    print(\"\\nVerificando números finais...\")\n",
    "    count_images_in_directory(train_dir, val_dir, test_dir)\n",
    "    \n",
    "    # 9. Gerar visualização das distribuições de imagens por paciente\n",
    "    print(\"\\nGerando visualização das distribuições...\")\n",
    "    plot_distribution(train_dir, val_dir, test_dir)\n",
    "    \n",
    "    # Salvar um arquivo de texto com a distribuição do dataset\n",
    "    with open('distribuicao_dataset.txt', 'w') as f:\n",
    "        f.write(f\"ALL: {len(all_images)} imagens de {len(np.unique(all_patients))} pacientes\\n\")\n",
    "        f.write(f\"HEM: {len(hem_images)} imagens de {len(np.unique(hem_patients))} pacientes\\n\\n\")\n",
    "        \n",
    "        f.write(\"Dividindo pacientes em conjuntos...\\n\")\n",
    "        f.write(f\"ALL - Treino: {len(all_train_patients)} pacientes, Validação: {len(all_val_patients)} pacientes, Teste: {len(all_test_patients)} pacientes\\n\")\n",
    "        f.write(f\"HEM - Treino: {len(hem_train_patients)} pacientes, Validação: {len(hem_val_patients)} pacientes, Teste: {len(hem_test_patients)} pacientes\\n\\n\")\n",
    "        \n",
    "        f.write(\"Distribuindo imagens originais...\\n\")\n",
    "        f.write(f\"ALL - Treino: {len(all_train_imgs)} imagens, Validação: {len(all_val_imgs)} imagens, Teste: {len(all_test_imgs)} imagens\\n\")\n",
    "        f.write(f\"HEM - Treino: {len(hem_train_imgs)} imagens, Validação: {len(hem_val_imgs)} imagens, Teste: {len(hem_test_imgs)} imagens\\n\\n\")\n",
    "        \n",
    "        # Incluir contagem final após augmentação\n",
    "        train_all_count = len([f for f in os.listdir(os.path.join(train_dir, 'all')) \n",
    "                              if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "        train_hem_count = len([f for f in os.listdir(os.path.join(train_dir, 'hem')) \n",
    "                              if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "        \n",
    "        val_all_count = len([f for f in os.listdir(os.path.join(val_dir, 'all')) \n",
    "                            if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "        val_hem_count = len([f for f in os.listdir(os.path.join(val_dir, 'hem')) \n",
    "                            if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "        \n",
    "        test_all_count = len([f for f in os.listdir(os.path.join(test_dir, 'all')) \n",
    "                             if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "        test_hem_count = len([f for f in os.listdir(os.path.join(test_dir, 'hem')) \n",
    "                             if f.endswith(('.jpg', '.png', '.bmp'))])\n",
    "        \n",
    "        f.write(\"Contagem final de imagens:\\n\")\n",
    "        f.write(f\"Treino - ALL: {train_all_count}, HEM: {train_hem_count}, Total: {train_all_count + train_hem_count}\\n\")\n",
    "        f.write(f\"Validação - ALL: {val_all_count}, HEM: {val_hem_count}, Total: {val_all_count + val_hem_count}\\n\")\n",
    "        f.write(f\"Teste - ALL: {test_all_count}, HEM: {test_hem_count}, Total: {test_all_count + test_hem_count}\\n\\n\")\n",
    "        \n",
    "        f.write(\"- Treino: dataset/processado/train\\n\")\n",
    "        f.write(\"- Validação: dataset/processado/validation\\n\")\n",
    "        f.write(\"- Teste: dataset/processado/test\")\n",
    "    \n",
    "    print(\"\\nPreparação do dataset concluída com sucesso!\")\n",
    "    \n",
    "    return train_dir, val_dir, test_dir\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    all_dir = \"dataset/all\"  # Diretório com imagens ALL (câncer)\n",
    "    hem_dir = \"dataset/hem\"  # Diretório com imagens HEM (saudáveis)\n",
    "    output_dir = \"dataset/processado\"  # Diretório de saída\n",
    "    \n",
    "    train_dir, val_dir, test_dir = create_dataset_preparation_pipeline(\n",
    "        all_dir=all_dir,\n",
    "        hem_dir=hem_dir,\n",
    "        output_dir=output_dir,\n",
    "        target_train_per_class=10000,  # 10.000 imagens por classe para treinamento\n",
    "        target_val_per_class=2000,     # 2.000 imagens por classe para validação\n",
    "        target_test_per_class=1000      # 1.000 imagens por classe para teste\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset preparado com sucesso:\")\n",
    "    print(f\"- Treino: {train_dir}\")\n",
    "    print(f\"- Validação: {val_dir}\")\n",
    "    print(f\"- Teste: {test_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
